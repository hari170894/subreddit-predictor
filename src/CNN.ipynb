{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_df = pd.read_csv('../res/data_all_train.csv', header=0)\n",
    "test_df = pd.read_csv('../res/data_all_test.csv', header=0)\n",
    "validation_df = pd.read_csv('../res/data_all_validate.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "labels=[]\n",
    "for label in train_df.subreddit:\n",
    "    labels.append(label)\n",
    "for doc in train_df.text:\n",
    "    unicode_string = nltk.word_tokenize(doc.decode('utf8'))\n",
    "    documents.append(unicode_string)\n",
    "    \n",
    "print(len(documents))\n",
    "\n",
    "print documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "def load_model():\n",
    "    glove_input_filename = '../res/glove.twitter.27B.100d.txt'\n",
    "    glove_output_filename = '../pickle_files/word2vec_glove.pkl'\n",
    "    # glove_filename = '../res/glove.840B.300d.txt'\n",
    "    import os.path\n",
    "    if not os.path.isfile(glove_output_filename):\n",
    "        word2vec_dict = {}\n",
    "        word2vec_dict = defaultdict(lambda: [0] * 100, word2vec_dict)\n",
    "\n",
    "        print(\"preparing to load word2vec file\")\n",
    "        with open(glove_input_filename) as f:\n",
    "            for line in tqdm(f, total=2196017):\n",
    "                love = line.split()\n",
    "                word, nums = love[0], [float(x) for x in love[1:]]\n",
    "                word2vec_dict[word] = nums\n",
    "\n",
    "    return word2vec_dict\n",
    "model =load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twod_doc_array=[]\n",
    "for doc in documents:\n",
    "    word_array=[]\n",
    "    i=0\n",
    "    while(i<len(doc) ):\n",
    "        if doc[i] in model:\n",
    "            word_array.append(list(model[doc[i]]))\n",
    "        i+=1\n",
    "    i=0\n",
    "    while (i<40):\n",
    "        word_array.append(np.zeros(100))\n",
    "        i+=1\n",
    "    \n",
    "    twod_doc_array.append(word_array[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twod_doc_array=np.array( twod_doc_array)\n",
    "\n",
    "print twod_doc_array.shape\n",
    "x=np.reshape(twod_doc_array,(twod_doc_array.shape[0],twod_doc_array.shape[1],twod_doc_array.shape[2],1))\n",
    "print x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    \n",
    "    # Input Layer\n",
    "\n",
    "    input_layer = tf.reshape(features['x'], [-1, 40, 100, 1])\n",
    "    input_layer =tf.cast(input_layer,dtype=tf.float32)\n",
    "\n",
    "    conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Convolutional Layer #2 and Pooling Layer #2\n",
    "    conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    conv3 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[4, 4],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "    \n",
    "    pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    conv4 = tf.layers.conv2d(\n",
    "      inputs=pool3,\n",
    "      filters=64,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "    \n",
    "    pool4 = tf.layers.max_pooling2d(inputs=conv4, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    conv5 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=64,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "    pool5 = tf.layers.max_pooling2d(inputs=conv5, pool_size=[2, 2], strides=2)\n",
    "    conv6 = tf.layers.conv2d(\n",
    "      inputs=pool5,\n",
    "      filters=64,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "    \n",
    "    pool6 = tf.layers.max_pooling2d(inputs=conv6, pool_size=[2, 2], strides=2)\n",
    "    # Dense Layer\n",
    "    \n",
    "    pool2_flat = tf.reshape(pool2, [-1, 10 * 25 * 64])\n",
    "    pool4_flat= tf.reshape(pool4, [-1, 10 * 25 * 64])\n",
    "    pool6_flat= tf.reshape(pool6, [-1, 10 * 25 * 64])\n",
    "    \n",
    "    poolflat=tf.concat([pool2_flat,pool4_flat,pool6_flat],0)\n",
    "    poolflat=tf.reshape(poolflat, [-1, 3 * 10 * 25 * 64])\n",
    "    print poolflat.get_shape().as_list()\n",
    "    dense = tf.layers.dense(inputs=poolflat, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(\n",
    "    inputs=dense, rate=0.2, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "    predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "      onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labellist=list(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print labellist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y=[]\n",
    "for i in labels:\n",
    "    y.append(labellist.index(i))\n",
    "print len(y)\n",
    "y=np.array(y)\n",
    "x=np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifier = tf.estimator.Estimator(\n",
    "model_fn=cnn_model_fn, model_dir=\"/tmp/pretrainedmode2\")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "  x={'x':x},\n",
    "  y=y,\n",
    "  batch_size=100,num_epochs=None,\n",
    "  shuffle=True)\n",
    "\n",
    "classifier.train(input_fn=train_input_fn,steps=5000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "end=time.time()\n",
    "print end-start\n",
    "start=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model and print results\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "  x={\"x\": x},\n",
    "  y=y,\n",
    "  num_epochs=1,\n",
    "  shuffle=False)\n",
    "eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "end=time.time()\n",
    "print end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testdocuments = []\n",
    "testlabels=[]\n",
    "for label in test_df.subreddit:\n",
    "    testlabels.append(label)\n",
    "for doc in test_df.text:\n",
    "    unicode_string = nltk.word_tokenize(doc.decode('utf8'))\n",
    "    testdocuments.append(unicode_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_twod_doc_array=[]\n",
    "for doc in testdocuments:\n",
    "    word_array=[]\n",
    "    i=0\n",
    "    while(i<len(doc) ):\n",
    "        if doc[i] in model:\n",
    "            word_array.append(list(model[doc[i]]))\n",
    "        i+=1\n",
    "    i=0\n",
    "    while (i<40):\n",
    "        word_array.append(np.zeros(100))\n",
    "        i+=1\n",
    "    \n",
    "    test_twod_doc_array.append(word_array[:40])\n",
    "\n",
    "test_twod_doc_array=np.array( test_twod_doc_array)\n",
    "print len(testdocuments)\n",
    "print test_twod_doc_array.shape\n",
    "testx=np.reshape(test_twod_doc_array,(test_twod_doc_array.shape[0],test_twod_doc_array.shape[1],test_twod_doc_array.shape[2],1))\n",
    "print testx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_y=[]\n",
    "for i in testlabels:\n",
    "    test_y.append(labellist.index(i))\n",
    "print len(test_y)\n",
    "test_y=np.array(test_y)\n",
    "testx=np.array(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print testx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate the model and print results\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "  x={\"x\": testx},\n",
    "  y=test_y,\n",
    "  num_epochs=1,\n",
    "  shuffle=False)\n",
    "eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
